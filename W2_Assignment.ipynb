{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Practice Lab: Neural Networks for Handwritten Digit Recognition, Multiclass \n",
    "\n",
    "In this exercise, you will use a neural network to recognize the hand-written digits 0-9.\n",
    "\n",
    "\n",
    "# Outline\n",
    "- [ 1 - Packages ](#1)\n",
    "    - [ 2 - ReLU Activation](#2)\n",
    "        - [ 3 - Softmax Function](#3)\n",
    "            - [ Exercise 1](#ex01)\n",
    "                - [ 4 - Neural Networks](#4)\n",
    "                    - [ 4.1 Problem Statement](#4.1)\n",
    "                        - [ 4.2 Dataset](#4.2)\n",
    "                            - [ 4.3 Model representation](#4.3)\n",
    "                                - [ 4.4 Tensorflow Model Implementation](#4.4)\n",
    "                                    - [ 4.5 Softmax placement](#4.5)\n",
    "                                        - [ Exercise 2](#ex02)\n"
   ],
   "id": "2323559fe2bbf2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Packages \n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment.\n",
    "- [numpy](https://numpy.org/) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a popular library to plot graphs in Python.\n",
    "- [tensorflow](https://www.tensorflow.org/) a popular platform for machine learning."
   ],
   "id": "b63f04d026c63a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T15:57:14.904770Z",
     "start_time": "2024-07-01T15:57:14.866336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearningstyle.mplstyle')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "from public_tests import *\n",
    "\n",
    "from autils import *\n",
    "from lab_utils_softmax import plt_softmax\n",
    "np.set_printoptions(precision=2)"
   ],
   "id": "3ddff20072b25cbe",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - ReLU Activation\n",
    "This week, a new activation was introduced, the Rectified Linear Unit (ReLU).\n",
    "$$ a = max(0,z) \\quad\\quad\\text {# ReLU function} $$"
   ],
   "id": "40300ce1f9bcdb07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T15:57:27.198907Z",
     "start_time": "2024-07-01T15:57:27.129294Z"
    }
   },
   "cell_type": "code",
   "source": "()",
   "id": "6a5207afb453c282",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt_act_trio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mplt_act_trio\u001B[49m()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'plt_act_trio' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<img align=\"right\" src=\"./images/C2_W2_ReLu.png\"     style=\" width:380px; padding: 10px 20px; \" >\n",
    "The example from the lecture on the right shows an application of the ReLU. In this example, the derived \"awareness\" feature is not binary but has a continuous range of values. The sigmoid is best for on/off or binary situations. The ReLU provides a continuous linear relationship. Additionally it has an 'off' range where the output is zero.\n",
    "The \"off\" feature makes the ReLU a Non-Linear activation. Why is this needed? This enables multiple units to contribute to to the resulting function without interfering. This is examined more in the supporting optional lab. "
   ],
   "id": "1c217d4c7d6b3027"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "53078c9f19bbef25"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
