{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Practice Lab: Neural Networks for Handwritten Digit Recognition, Multiclass \n",
    "\n",
    "In this exercise, you will use a neural network to recognize the hand-written digits 0-9.\n",
    "\n",
    "\n",
    "# Outline\n",
    "- [ 1 - Packages ](#1)\n",
    "    - [ 2 - ReLU Activation](#2)\n",
    "        - [ 3 - Softmax Function](#3)\n",
    "            - [ Exercise 1](#ex01)\n",
    "                - [ 4 - Neural Networks](#4)\n",
    "                    - [ 4.1 Problem Statement](#4.1)\n",
    "                        - [ 4.2 Dataset](#4.2)\n",
    "                            - [ 4.3 Model representation](#4.3)\n",
    "                                - [ 4.4 Tensorflow Model Implementation](#4.4)\n",
    "                                    - [ 4.5 Softmax placement](#4.5)\n",
    "                                        - [ Exercise 2](#ex02)\n"
   ],
   "id": "2323559fe2bbf2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Packages \n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment.\n",
    "- [numpy](https://numpy.org/) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a popular library to plot graphs in Python.\n",
    "- [tensorflow](https://www.tensorflow.org/) a popular platform for machine learning."
   ],
   "id": "b63f04d026c63a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T16:04:38.707799Z",
     "start_time": "2024-07-01T16:04:38.683187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('./deeplearningstyle.mplstyle')\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "from public_tests import *\n",
    "\n",
    "from autils import *\n",
    "from lab_utils_softmax import plt_softmax\n",
    "np.set_printoptions(precision=2)"
   ],
   "id": "3ddff20072b25cbe",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - ReLU Activation\n",
    "This week, a new activation was introduced, the Rectified Linear Unit (ReLU).\n",
    "$$ a = max(0,z) \\quad\\quad\\text {# ReLU function} $$"
   ],
   "id": "40300ce1f9bcdb07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T16:04:38.712544Z",
     "start_time": "2024-07-01T16:04:38.709381Z"
    }
   },
   "cell_type": "code",
   "source": "##plt_act_trio()",
   "id": "6a5207afb453c282",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<img align=\"right\" src=\"./images/C2_W2_ReLu.png\"     style=\" width:380px; padding: 10px 20px; \" >\n",
    "The example from the lecture on the right shows an application of the ReLU. In this example, the derived \"awareness\" feature is not binary but has a continuous range of values. The sigmoid is best for on/off or binary situations. The ReLU provides a continuous linear relationship. Additionally it has an 'off' range where the output is zero.\n",
    "The \"off\" feature makes the ReLU a Non-Linear activation. Why is this needed? This enables multiple units to contribute to to the resulting function without interfering. This is examined more in the supporting optional lab. "
   ],
   "id": "1c217d4c7d6b3027"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<img align=\"right\" src=\"./images/C2_W2_ReLu.png\"     style=\" width:380px; padding: 10px 20px; \" >\n",
    "The example from the lecture on the right shows an application of the ReLU. In this example, the derived \"awareness\" feature is not binary but has a continuous range of values. The sigmoid is best for on/off or binary situations. The ReLU provides a continuous linear relationship. Additionally it has an 'off' range where the output is zero.     \n",
    "The \"off\" feature makes the ReLU a Non-Linear activation. Why is this needed? This enables multiple units to contribute to to the resulting function without interfering. This is examined more in the supporting optional lab. "
   ],
   "id": "386a91178cb009c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Softmax Function\n",
    "A multiclass neural network generates N outputs. One output is selected as the predicted answer. In the output layer, a vector $\\mathbf{z}$ is generated by a linear function which is fed into a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will sum to 1. They can be interpreted as probabilities. The larger inputs to the softmax will correspond to larger output probabilities.\n",
    "<center>  <img  src=\"./images/C2_W2_NNSoftmax.PNG\" width=\"600\" />  "
   ],
   "id": "1028ee873e4fec50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The softmax function can be written:\n",
    "$$a_j = \\frac{e^{z_j}}{ \\sum_{k=0}^{N-1}{e^{z_k} }} \\tag{1}$$\n",
    "\n",
    "Where $z = \\mathbf{w} \\cdot \\mathbf{x} + b$ and N is the number of feature/categories in the output layer.  "
   ],
   "id": "513fd482debdf14e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"ex01\"></a>\n",
    "### Exercise 1\n",
    "Let's create a NumPy implementation:"
   ],
   "id": "b7f02451bb8e01c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T16:04:38.728957Z",
     "start_time": "2024-07-01T16:04:38.724055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# UNQ_C1\n",
    "# GRADED CELL: my_softmax\n",
    "\n",
    "def my_softmax(z):\n",
    "    \"\"\" Softmax converts a vector of values to a probability distribution.\n",
    "    Args:\n",
    "      z (ndarray (N,))  : input data, N features\n",
    "    Returns:\n",
    "      a (ndarray (N,))  : softmax of z\n",
    "    \"\"\"\n",
    "\n",
    "    ez = np.exp(z)\n",
    "    a = ez/np.sum(ez)\n",
    "\n",
    "    return a"
   ],
   "id": "1fb69e94d51c2d93",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T16:05:02.431544Z",
     "start_time": "2024-07-01T16:05:02.403489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "z = np.array([1., 2., 3., 4.])\n",
    "a = my_softmax(z)\n",
    "atf = tf.nn.softmax(z)\n",
    "print(f\"my_softmax(z):         {a}\")\n",
    "print(f\"tensorflow softmax(z): {atf}\")\n"
   ],
   "id": "fc28ace4f92f703e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_softmax(z):         [0.03 0.09 0.24 0.64]\n",
      "tensorflow softmax(z): [0.03 0.09 0.24 0.64]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "939d6b858447ed30",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
